{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Time with Markov Twain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text and Getting Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using JLD;\n",
    "using ForwardDiff;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two workers to do processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# addprocs(2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, Julia interpreter prints last variable of cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function clean_corpus(text, regex; normalize = true, lower_case = true)\n",
    "    if normalize\n",
    "        # replace control characters with spaces\n",
    "        text = normalize_string(text, stripmark = true, stripignore = true, stripcc = true)\n",
    "    end\n",
    "    \n",
    "    if lower_case\n",
    "        text = lowercase(text)\n",
    "    end\n",
    "    \n",
    "    # remove unwanted characters\n",
    "    text = replace(text, regex, \"\")\n",
    "    \n",
    "    # remove \"\"\n",
    "    text = split(text)\n",
    "    target_index = 1\n",
    "    for i in 1:length(text)\n",
    "        target_index = findnext(text, \"\", target_index)\n",
    "        if target_index == 0\n",
    "            break\n",
    "        else\n",
    "            splice!(text, target_index)\n",
    "        end\n",
    "    end        \n",
    "    text = join(text, \" \")\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(\"mark_twain_books/adventures_of_tom_sawyer.txt\")\n",
    "ats = readall(f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create regex object (I prefer whitelisting characters I want to keep)\n",
    "chars_to_remove = r\"[^a-z ]\"\n",
    "ats_clean = clean_corpus(ats, chars_to_remove);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define text to numeric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function text_to_numeric(text, symbols)\n",
    "    numeric_text = []\n",
    "    for each in text\n",
    "        push!(numeric_text, findfirst(symbols, each))\n",
    "    end\n",
    "\n",
    "    numeric_text\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function numeric_to_text(numeric, symbols)\n",
    "    text= []\n",
    "    for num in numeric\n",
    "        push!(text, symbols[num])\n",
    "    end\n",
    "    text\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function get_corpus_frequencies(corpus, ngram; groupby = \"words\")\n",
    "    # to get frequency of symbol x after ngram symbols\n",
    "    ngram = ngram + 1\n",
    "    if groupby == \"chars\"\n",
    "        corpus = split(corpus, \"\")\n",
    "    else        \n",
    "        corpus = split(corpus)\n",
    "    end\n",
    "    \n",
    "    # find unique symbols\n",
    "    unique_symbols = unique(corpus)   \n",
    "    # convert text to numbers\n",
    "    corpus_numeric = text_to_numeric(corpus, unique_symbols);\n",
    "    # create M\n",
    "    dimensions = repeat([length(unique_symbols)], outer=[ngram])\n",
    "    M = repeat(zeros(UInt16, 1), outer = dimensions)\n",
    "    # get frequencies for ngram of text\n",
    "    for i in 1:length(corpus)-ngram+1\n",
    "        M[corpus_numeric[i:i+ngram-1]...] += 1\n",
    "    end\n",
    "    \n",
    "    M\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this frequency array works on a subset of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_ats_clean = length(split(ats_clean))\n",
    "# text subset\n",
    "ats_subset = join(split(ats_clean)[1:round(Int64, len_ats_clean/2)], \" \")\n",
    "@time M = get_corpus_frequencies(ats_subset, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine all three Mark Twain novels and create a frequency array for the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import other books\n",
    "f = open(\"mark_twain_books/huckleberry_finn.txt\")\n",
    "hf = readall(f)\n",
    "f = open(\"mark_twain_books/the_prince_and_the_pauper.txt\")\n",
    "tpatp = readall(f)\n",
    "\n",
    "# clean other books\n",
    "hf_clean = clean_corpus(hf, chars_to_remove)\n",
    "tpatp_clean = clean_corpus(tpatp, chars_to_remove)\n",
    "\n",
    "# combine all books\n",
    "big_corpus_clean = ats_clean * \" \" * hf_clean * \" \" * tpatp_clean\n",
    "M_1 = get_corpus_frequencies(big_corpus_clean, 1);\n",
    "save(\"M_1.jld\", \"M_1\", M_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For arrays with 64-bit entries and ngram = 2:**   \n",
    "Since I only have ~29 GB of available memory, I need to figure out how many unique symbols I can afford.  \n",
    "By solving the following inequality, I figure I need to keep the number of unique_symbols < 1550.  \n",
    "$x^3*64/8/1024^3 < 29$  \n",
    "At 1550 unqiue symbols for ngram = 2, ~23 GB of memory should be used, so that's what we'll try here.\n",
    "\n",
    "**For arrays with unsigned 16-bit entries:**  \n",
    "The inequality, $x^3*16/8/1024^3 < 29$, would allow me to have approximately ~2500 unique symbols. But for demo purposes, I'm just going to use 1452 unique symbols to conserve memory.\n",
    "\n",
    "For ngram = 3 and with the constraint of memory, we'd be looking at only low hundreds of unique words instead of thousands. As a result, we're only going to consider cases of ngram = 1 and ngram = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or take combo of half of each book if ngram = 2 due to memory restrictions\n",
    "len_ats_clean = length(split(ats_clean))\n",
    "ats_subset = join(split(ats_clean)[1:round(Int64, len_ats_clean/45)], \" \")\n",
    "len_hf_clean = length(split(hf_clean))\n",
    "hf_subset = join(split(hf_clean)[1:round(Int64, len_hf_clean/45)], \" \")\n",
    "len_tpatp_clean = length(split(tpatp_clean))\n",
    "tpatp_subset = join(split(tpatp_clean)[1:round(Int64, len_tpatp_clean/45)], \" \")\n",
    "sub_corpus_clean = ats_subset * \" \" * hf_subset * \" \" * tpatp_subset;\n",
    "\n",
    "M_2 = get_corpus_frequencies(sub_corpus_clean, 2)\n",
    "save(\"M_2.jld\", \"M_2\", M_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import frequency objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M_1 = load(\"M_1.jld\", \"M_1\")\n",
    "M_2 = load(\"M_2.jld\", \"M_2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function choose_next_state(distribution, r)\n",
    "    # only consider entries that are non-zero\n",
    "    nonzero_entries = findn(distribution)\n",
    "    # if there was no probability jumping to another state,\n",
    "    # pick a random state\n",
    "    if length(nonzero_entries) == 0\n",
    "        return rand(1:length(distribution))\n",
    "    end\n",
    "    distribution_nonzero = distribution[nonzero_entries]\n",
    "    ranges = cumsum(distribution_nonzero)\n",
    "    \n",
    "    for (idx, range) in enumerate(ranges)\n",
    "        if r < range\n",
    "            return nonzero_entries[idx]\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function markov_model(ϕ, num_steps, unique_symbols, ngram, M, groupby)\n",
    "\n",
    "    if groupby == \"chars\"\n",
    "        ϕ = split(ϕ, \"\")\n",
    "    else\n",
    "        ϕ = split(ϕ)\n",
    "    end\n",
    "    \n",
    "    # create empty array to store result of Markov jumping from state to state\n",
    "    markov_chain_text = []\n",
    "    append!(markov_chain_text, ϕ)\n",
    "    \n",
    "    current_state = text_to_numeric(ϕ, unique_symbols)\n",
    "\n",
    "    for step in 1:num_steps\n",
    "        # normalize row (this division automatically converts from UInt16 to Float64)\n",
    "        distribution = M[current_state..., :][:] / sum(M[current_state..., :][:])\n",
    "\n",
    "        # randomly choose next word\n",
    "        # generate random number betweeen 0 and 1\n",
    "        r = rand()\n",
    "        next_word_idx = choose_next_state(distribution, r)\n",
    "        next_word = numeric_to_text([next_word_idx], unique_symbols)[1]\n",
    "        push!(markov_chain_text, next_word)\n",
    "        current_state = text_to_numeric(markov_chain_text[end-ngram+1:end], unique_symbols)\n",
    "    end\n",
    "    \n",
    "    markov_chain_text\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function get_phi(cleaned_corpus, ngram; groupby = \"words\")\n",
    "    if groupby == \"chars\"\n",
    "        cleaned_corpus_array = split(cleaned_corpus, \"\")\n",
    "    else\n",
    "        cleaned_corpus_array = split(cleaned_corpus)\n",
    "        \n",
    "    end\n",
    "    starting_point = rand(1:length(cleaned_corpus_array)-ngram)\n",
    "    ϕ = join(cleaned_corpus_array[starting_point:starting_point+ngram-1], \" \") \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function run(corpus, M; num_steps = 10, ngram = 2, groupby = \"words\")\n",
    "    @show ngram\n",
    "    unique_symbols = unique(split(corpus))\n",
    "    # choose random ngram set of symbols from text\n",
    "    ϕ = get_phi(corpus, ngram, groupby = groupby)\n",
    "    @show ϕ\n",
    "\n",
    "    markov_chain_text = markov_model(ϕ, num_steps, unique_symbols, ngram, M, groupby)\n",
    "    join(markov_chain_text, \" \")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram = 1\n",
      "ϕ = \"if\"\n",
      "if there was crammed it looked late tonight i never heard jim didnt look at any sort of it and meet that stands all of this might be conscious that you better than what to see no more then let up behind him he turns to be the duke with his lip and threaten it you know how he didnt give my jacket quick work till the little one unweighty trifle meantime across her asprawlin den and told me he not be sorry to see me to eat of the money as soon have allowed another shake them presently as the\n",
      "ngram = 2\n",
      "ϕ = \"and after\"\n",
      "and after that every time i heard an owl away off whowhooing about somebody that was going to set still and lonesome as soon as tom was back we cut along the path around the maypole in cheapside and at the period of this storythat is to say thirty or forty years ago although my book is intended mainly for the house was all as still as death now and so it made the cold shivers run over me then away out in the part where tom canty and to regard him as a boy they were not organised they were goodhearted\n"
     ]
    }
   ],
   "source": [
    "ngram1results = run(big_corpus_clean, M_1, num_steps = 100, ngram = 1)\n",
    "println(ngram1results)\n",
    "ngram2results = run(sub_corpus_clean, M_2, num_steps = 100, ngram = 2)\n",
    "println(ngram2results)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
