{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Time with Markov Twain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Text and Getting Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using JLD;\n",
    "using ForwardDiff;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two workers to do processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# addprocs(2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, Julia interpreter prints last variable of cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function clean_corpus(text, regex; normalize = true, lower_case = true)\n",
    "    if normalize\n",
    "        # replace control characters with spaces\n",
    "        text = normalize_string(text, stripmark = true, stripignore = true, stripcc = true)\n",
    "    end\n",
    "    \n",
    "    if lower_case\n",
    "        text = lowercase(text)\n",
    "    end\n",
    "    \n",
    "    # remove unwanted characters\n",
    "    text = replace(text, regex, \"\")\n",
    "    \n",
    "    # remove \"\"\n",
    "    text = split(text)\n",
    "    target_index = 1\n",
    "    for i in 1:length(text)\n",
    "        target_index = findnext(text, \"\", target_index)\n",
    "        if target_index == 0\n",
    "            break\n",
    "        else\n",
    "            splice!(text, target_index)\n",
    "        end\n",
    "    end        \n",
    "    text = join(text, \" \")\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = open(\"mark_twain_books/adventures_of_tom_sawyer.txt\")\n",
    "ats = readall(f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create regex object (I prefer whitelisting characters I want to keep)\n",
    "chars_to_remove = r\"[^a-z ]\"\n",
    "ats_clean = clean_corpus(ats, chars_to_remove);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define text to numeric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function text_to_numeric(text, symbols)\n",
    "    numeric_text = []\n",
    "    for each in text\n",
    "        push!(numeric_text, findfirst(symbols, each))\n",
    "    end\n",
    "\n",
    "    numeric_text\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function numeric_to_text(numeric, symbols)\n",
    "    text= []\n",
    "    for num in numeric\n",
    "        push!(text, symbols[num])\n",
    "    end\n",
    "    text\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function get_corpus_frequencies(corpus, ngram; groupby = \"words\")\n",
    "    # to get frequency of symbol x after ngram symbols\n",
    "    ngram = ngram + 1\n",
    "    if groupby == \"chars\"\n",
    "        corpus = split(corpus, \"\")\n",
    "    else        \n",
    "        corpus = split(corpus)\n",
    "    end\n",
    "    \n",
    "    # find unique symbols\n",
    "    unique_symbols = unique(corpus)   \n",
    "    # convert text to numbers\n",
    "    corpus_numeric = text_to_numeric(corpus, unique_symbols);\n",
    "    # create M\n",
    "    dimensions = repeat([length(unique_symbols)], outer=[ngram])\n",
    "    M = repeat([0], outer = dimensions)\n",
    "    # get frequencies for ngram of text\n",
    "    for i in 1:length(corpus)-ngram+1\n",
    "        M[corpus_numeric[i:i+ngram-1]...] += 1\n",
    "    end\n",
    "    \n",
    "    M\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this frequency array works on a subset of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_ats_clean = length(split(ats_clean))\n",
    "# text subset\n",
    "ats_subset = join(split(ats_clean)[1:round(Int64, len_ats_clean/2)], \" \")\n",
    "@time M = get_corpus_frequencies(ats_subset, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine all three Mark Twain novels and create a frequency array for the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import other books\n",
    "f = open(\"mark_twain_books/huckleberry_finn.txt\")\n",
    "hf = readall(f)\n",
    "f = open(\"mark_twain_books/the_prince_and_the_pauper.txt\")\n",
    "tpatp = readall(f)\n",
    "\n",
    "# clean other books\n",
    "hf_clean = clean_corpus(hf, chars_to_remove)\n",
    "tpatp_clean = clean_corpus(tpatp, chars_to_remove)\n",
    "\n",
    "# combine all books\n",
    "big_corpus_clean = ats_clean * \" \" * hf_clean * \" \" * tpatp_clean\n",
    "# M_1 = get_corpus_frequencies(big_corpus_clean, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this on desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @time M_3 = get_corpus_frequencies(big_corpus_clean, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I only have 31.4 GB of memory, unique_symbols must be < 1615 for ngram = 2.  \n",
    "At 1452 unqiue_symbols for ngram = 2, ~23 GB of memory should be used, so that's what we'll try here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or take combo of half of each book if ngram = 2 due to memory restrictions\n",
    "len_ats_clean = length(split(ats_clean))\n",
    "ats_subset = join(split(ats_clean)[1:round(Int64, len_ats_clean/45)], \" \")\n",
    "len_hf_clean = length(split(hf_clean))\n",
    "hf_subset = join(split(hf_clean)[1:round(Int64, len_hf_clean/45)], \" \")\n",
    "len_tpatp_clean = length(split(tpatp_clean))\n",
    "tpatp_subset = join(split(tpatp_clean)[1:round(Int64, len_tpatp_clean/45)], \" \")\n",
    "sub_corpus_clean = ats_subset * \" \" * hf_subset * \" \" * tpatp_subset\n",
    "\n",
    "# M_2 = get_corpus_frequencies(sub_corpus_clean, 2)\n",
    "# save(\"M_2.jld\", \"M_2\", M_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import frequency objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M_1 = load(\"M_1.jld\", \"M_1\")\n",
    "M_2 = load(\"M_2.jld\", \"M_2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function choose_next_state(distribution, r)\n",
    "    # only consider entries that are non-zero\n",
    "    nonzero_entries = findn(distribution)\n",
    "    # if there was no probability jumping to another state,\n",
    "    # pick a random state\n",
    "    if length(nonzero_entries) == 0\n",
    "        return rand(1:length(distribution))\n",
    "    end\n",
    "    distribution_nonzero = distribution[nonzero_entries]\n",
    "    ranges = cumsum(distribution_nonzero)\n",
    "    \n",
    "    for (idx, range) in enumerate(ranges)\n",
    "        if r < range\n",
    "            return nonzero_entries[idx]\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function markov_model(ϕ, num_steps, unique_symbols, ngram, M, groupby)\n",
    "\n",
    "    if groupby == \"chars\"\n",
    "        ϕ = split(ϕ, \"\")\n",
    "    else\n",
    "        ϕ = split(ϕ)\n",
    "    end\n",
    "    \n",
    "    # create empty array to store result of Markov jumping from state to state\n",
    "    markov_chain_text = []\n",
    "    append!(markov_chain_text, ϕ)\n",
    "    \n",
    "    current_state = text_to_numeric(ϕ, unique_symbols)\n",
    "\n",
    "    for step in 1:num_steps\n",
    "        # normalize row\n",
    "        distribution = M[current_state..., :][:] / sum(M[current_state..., :][:])\n",
    "\n",
    "        # randomly choose next word\n",
    "        # generate random number betweeen 0 and 1\n",
    "        r = rand()\n",
    "        next_word_idx = choose_next_state(distribution, r)\n",
    "        next_word = numeric_to_text([next_word_idx], unique_symbols)[1]\n",
    "        push!(markov_chain_text, next_word)\n",
    "        current_state = text_to_numeric(markov_chain_text[end-ngram+1:end], unique_symbols)\n",
    "    end\n",
    "    \n",
    "    markov_chain_text\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function get_phi(cleaned_corpus, ngram; groupby = \"words\")\n",
    "    if groupby == \"chars\"\n",
    "        cleaned_corpus_array = split(cleaned_corpus, \"\")\n",
    "    else\n",
    "        cleaned_corpus_array = split(cleaned_corpus)\n",
    "        \n",
    "    end\n",
    "    starting_point = rand(1:length(cleaned_corpus_array)-ngram)\n",
    "    ϕ = join(cleaned_corpus_array[starting_point:starting_point+ngram-1], \" \") \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function run(corpus, M; num_steps = 10, ngram = 2, groupby = \"words\")\n",
    "\n",
    "    unique_symbols = unique(split(corpus))\n",
    "    # choose random ngram set of symbols from text\n",
    "    ϕ = get_phi(corpus, ngram, groupby = groupby)\n",
    "    @show ϕ\n",
    "\n",
    "    markov_chain_text = markov_model(ϕ, num_steps, unique_symbols, ngram, M, groupby)\n",
    "    join(markov_chain_text, \" \")\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ϕ = \"young people\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"young people began to find an added value in it because of\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run(big_corpus_clean, M_1, num_steps = 10, ngram = 1, groupby = \"words\")\n",
    "run(sub_corpus_clean, M_2, ngram = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
