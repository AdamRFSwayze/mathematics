{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Bobby Lindsey, Adam Swayze, Wesley Pryor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components Analysis (PCA for short) is a technique used to reduce the dimensions of a data set. There are a few helpful ways to explain what PCA does:\n",
    "* PCA computes the most meaningful basis to re-express a noisy, garbled data set.\n",
    "* PCA yields a feature subspace that maximizes the variance along the axes.\n",
    "* PCA reduces the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors (which all have the same unit length of 1) will form the axes.\n",
    "\n",
    "PCA is an extremely useful technique in data mining. Hi-resolution images can have thousands of dimensions (MRI scans are huge) and those familiar with \"the curse of dimensionality\" know that certain algorithms are only approachable when one's data set is of a manageable size. PCA employs a combination of topics like correlation, eigenvalues, and eigenvectors to determine which variables account for most of the variability in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a transformation $\\mathrm{T} =\n",
    "\\begin{bmatrix}\n",
    "7 & -1\\\\\n",
    "-1 & 7\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "To find the eigenvalues, $\\lambda$s, of the transformation:\n",
    "* $\\mathrm{T}x = \\lambda x$\n",
    "* $\\mathrm{T}x = \\lambda \\mathrm{I} x$\n",
    "* $\\mathrm{T}x - \\lambda \\mathrm{I} x = 0$\n",
    "* $(\\mathrm{T} - \\lambda \\mathrm{I})x = 0 : x \\ne 0$ ```part of the definition of an eigenvector```\n",
    "* $x \\in \\mathrm{ker}(\\mathrm{T} - \\lambda \\mathrm{I})$ ```the kernel has a solution iff its solution isn't the trivial one; we can express this with a determinant```\n",
    "* $\\mathrm{det}(\\mathrm{T} - \\lambda \\mathrm{I}) = 0$ ```this is called the characteristic equation```\n",
    "* $\\mathrm{det}(\\lambda \\mathrm{I} - \\mathrm{T}) = 0$\n",
    "* $\\mathrm{det}\\big(\\lambda \\big(\\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}\\big) - \\begin{bmatrix} 7 & -1\\\\ -1 & 7 \\end{bmatrix}\\big) = 0$\n",
    "* $\\mathrm{det}\\big(\\begin{bmatrix} \\lambda & 0\\\\ 0 & \\lambda \\end{bmatrix} - \\begin{bmatrix} 7 & -1\\\\ -1 & 7 \\end{bmatrix}\\big) = 0$\n",
    "* $\\mathrm{det}\\big(\\begin{bmatrix} \\lambda-7 & 0\\\\ 0 & \\lambda-7 \\end{bmatrix}\\big) = 0$\n",
    "* $(\\lambda - 7)^2 - 1 = 0$\n",
    "* $\\lambda^2 - 14 \\lambda + 49 - 1 = 0$\n",
    "* $\\lambda^2 - 14 \\lambda + 48 = 0$\n",
    "* $(\\lambda - 6)(\\lambda - 8) = 0$\n",
    "* $\\lambda = 6, \\lambda = 8$\n",
    "\n",
    "To find the eigenvectors associated with the eigenvalues, we need only plug each eigenvalue into the characteristic equation and solve for the vector or vectors that provide the solution to the equation:\n",
    "* $\\lambda = 6$\n",
    "* $\\mathrm{det}(6 \\mathrm{I} - \\mathrm{T}) = 0$\n",
    "* $\\begin{bmatrix} -1 & 1\\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} a\\\\ b \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n",
    "* rref(above) and solve for pivot variables to get the following set of vectors as a solution - $\\big\\{\\begin{bmatrix} a \\\\ a \\end{bmatrix} : a \\in \\mathbb{R}\\big\\} = \\big\\{a\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} : a \\in \\mathbb{R}\\big\\} = \\mathrm{span}\\big(\\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\big)$\n",
    "* We can repeat the above steps with $\\lambda = 8$ and get the set of vectors - $\\mathrm{span}\\big(\\begin{bmatrix} 1 \\\\ -1\\end{bmatrix}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues: [ 8.  6.]\n",
      "\n",
      "normalized eigenvectors:\n",
      " [[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "\n",
      "un-normalized eigenvectors:\n",
      " [[ 1.  1.]\n",
      " [-1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "# create a numpy matrix for our transformation\n",
    "T = np.array([[7, -1], [-1, 7]])\n",
    "# get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = LA.eig(T)\n",
    "print(\"eigenvalues: %s\\n\" %eigenvalues)\n",
    "# eigenvectors will be normalized\n",
    "print(\"normalized eigenvectors:\\n %s\\n\" %eigenvectors)\n",
    "#print(eigenvectors)\n",
    "# un-normalized eigenvectors\n",
    "normalized_eigenvectors = eigenvectors*math.sqrt(2)\n",
    "print(\"un-normalized eigenvectors:\\n %s\" %normalized_eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA the Hard Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalize data set (always a good idea to do so that certain algorithms aren't affected by the scale of different variables)\n",
    "2. Calculate the correlation matrix for your dataset. This matrix will be your transformation and will provide the correlation between each pair of variables. (We use correlation instead of covariance since correlation is a normalized version of the covariance matrix; although we're not too concerned about normalization since we already normalized the data set in step 1).\n",
    "3. Find the eigenvectors and eigenvalues of that matrix using the steps above.\n",
    "4. For each eigenvalue, take the absolute value and divide it by the sum of all the eigenvalues which will provide the proportion of variance that its associated eigenvector contributes to the data.\n",
    "5. Sort the principal components by their associated eigenvalues (highest to lowest).\n",
    "6. Eigenvalues tell you which principal components to keep. The number of principal components to keep will be based on the cumulative explained variance that the eigenvalues account for. The cumulative explained variance to stop at is a threshold that is decided by how much variability you want to explain.\n",
    "7. Stick each eigenvector you want to keep in a matrix (this is called a \"feature vector\"). We call each eigenvector a \"principal component\".\n",
    "8. Now project the data set onto the new feature space (i.e. the feature vector) by multiplying your data set by the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "[[ 1.         -0.10936925  0.87175416  0.81795363]\n",
      " [-0.10936925  1.         -0.4205161  -0.35654409]\n",
      " [ 0.87175416 -0.4205161   1.          0.9627571 ]\n",
      " [ 0.81795363 -0.35654409  0.9627571   1.        ]]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
      " [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
      " [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
      " [ 0.56561105 -0.06541577  0.6338014   0.52354627]]\n",
      "\n",
      "Eigenvalues in descending order:\n",
      "2.91081808375\n",
      "0.921220930707\n",
      "0.147353278305\n",
      "0.0206077072356\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "print(\"Original data set:\")\n",
    "display(df.tail())\n",
    "# normalize data set\n",
    "data_set = df.ix[:,0:3].values\n",
    "data_set = StandardScaler().fit_transform(data_set)\n",
    "# calculate correlation matrix\n",
    "correlation_matrix = np.corrcoef(data_set.T)\n",
    "print(\"Correlation matrix:\\n%s\\n\" %correlation_matrix)\n",
    "# find eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = LA.eig(correlation_matrix)\n",
    "print(\"Eigenvectors:\\n%s\\n\" %eigenvectors)\n",
    "# make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n",
    "# sort the (eigenvalue, eigenvector) tuples from highest to lowest\n",
    "eigen_pairs.sort()\n",
    "eigen_pairs.reverse()\n",
    "print('Eigenvalues in descending order:')\n",
    "for pair in eigen_pairs:\n",
    "    print(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727704520938\n",
      "0.230305232677\n",
      "0.0368383195763\n",
      "0.00515192680891\n"
     ]
    }
   ],
   "source": [
    "# find explained variance of eigenvalues\n",
    "eigenvalues_sum = sum(eigenvalues)\n",
    "explained_variance = [(eigenvalue / eigenvalues_sum) for eigenvalue in sorted(eigenvalues, reverse=True)]\n",
    "for i in explained_variance:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the first two eigenvectors account for the majority of the variance in the data set. Let's just use the two eigenvectors associated with those first two eigenvalues, stick them in a matrix, and apply said matrix to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionally-reduced data set (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870522</td>\n",
       "      <td>-0.382822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.558492</td>\n",
       "      <td>0.905314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.520845</td>\n",
       "      <td>-0.266795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.376391</td>\n",
       "      <td>-1.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.959299</td>\n",
       "      <td>0.022284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "145  1.870522 -0.382822\n",
       "146  1.558492  0.905314\n",
       "147  1.520845 -0.266795\n",
       "148  1.376391 -1.016362\n",
       "149  0.959299  0.022284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# put eigenvectors in matrix\n",
    "feature_vector = np.hstack((eigen_pairs[0][1].reshape(4,1), \n",
    "                            eigen_pairs[1][1].reshape(4,1)))\n",
    "# project data set onto feature_vector\n",
    "pca_data_set = pd.DataFrame(data_set.dot(feature_vector))\n",
    "print(\"Dimensionally-reduced data set (tail):\")\n",
    "display(pca_data_set.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA the Easy Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the mechanics of Principal Components Analysis, we can use convenience functions in sklearn to do all the heavy lifing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix:\n",
      "[[ 1.         -0.10936925  0.87175416  0.81795363]\n",
      " [-0.10936925  1.         -0.4205161  -0.35654409]\n",
      " [ 0.87175416 -0.4205161   1.          0.9627571 ]\n",
      " [ 0.81795363 -0.35654409  0.9627571   1.        ]]\n",
      "\n",
      "Eigenvectors:\n",
      "[[ 0.52237162  0.37231836 -0.72101681 -0.26199559]\n",
      " [-0.26335492  0.92555649  0.24203288  0.12413481]\n",
      " [ 0.58125401  0.02109478  0.14089226  0.80115427]\n",
      " [ 0.56561105  0.06541577  0.6338014  -0.52354627]]\n",
      "\n",
      "Explained Variance:\n",
      "[ 2.91081808  0.92122093  0.14735328  0.02060771]\n",
      "\n",
      "Dimensionally-reduced data set (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.870522</td>\n",
       "      <td>0.382822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.558492</td>\n",
       "      <td>-0.905314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.520845</td>\n",
       "      <td>0.266795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.376391</td>\n",
       "      <td>1.016362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.959299</td>\n",
       "      <td>-0.022284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1\n",
       "145  1.870522  0.382822\n",
       "146  1.558492 -0.905314\n",
       "147  1.520845  0.266795\n",
       "148  1.376391  1.016362\n",
       "149  0.959299 -0.022284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scikit-learn convenience command\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# standardize data set (sklearn uses covariance matrix instead of correlation matrix)\n",
    "data_set = df.ix[:,0:3].values\n",
    "data_set_std = StandardScaler().fit_transform(data_set)\n",
    "sklearn_pca = sklearnPCA(n_components=4)\n",
    "sklearn_pca.fit(data_set_std)\n",
    "# covariance matrix\n",
    "covariance_matrix = sklearn_pca.get_covariance()\n",
    "print(\"Covariance matrix:\\n%s\\n\" %covariance_matrix)\n",
    "# eigenvectors\n",
    "eigenvectors = sklearn_pca.components_.T\n",
    "print(\"Eigenvectors:\\n%s\\n\" %eigenvectors)\n",
    "# eigenvalues\n",
    "explained_variance = sklearn_pca.explained_variance_\n",
    "print(\"Explained Variance:\\n%s\\n\" %explained_variance)\n",
    "\n",
    "# explained variance shows we really only need the eigenvectors associated with the first two eigenvalues\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "sklearn_pca.fit(data_set_std)\n",
    "# apply the feature vector to the data set\n",
    "pca_data_set = pd.DataFrame(sklearn_pca.transform(data_set_std))\n",
    "print(\"Dimensionally-reduced data set (tail):\")\n",
    "display(pca_data_set.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://plot.ly/ipython-notebooks/principal-component-analysis/#Standardizing\n",
    "* http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
